# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Red Hat Inc.
# This file is distributed under the same license as the Ansible Tower
# Administration Guide package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Ansible Tower Administration Guide 3.1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-03-08 12:30-0500\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 1.3\n"

#: ../../source/clustering.rst:4
msgid "Clustering"
msgstr ""

#: ../../source/clustering.rst:6
msgid ""
"|at| 3.1 introduces Clustering as an alternate approach to redundancy, "
"replacing the redundancy solution configured with the active-passive "
"nodes that involves primary and secondary instances. For versions older "
"than 3.1, refer to the older versions of this chapter of the "
"*Administration Guide*."
msgstr ""

#: ../../source/clustering.rst:8
msgid ""
"Clustering is sharing load between hosts. Each node should be able to act"
" as an entrypoint for UI and API access. This should enable Tower "
"administrators to use load balancers in front of as many nodes as they "
"wish and maintain good data visibility. Each node should be able to join "
"the Tower cluster and expand its ability to execute jobs. This is "
"currently a naive system where jobs can and will run anywhere rather than"
" be directed on where to run."
msgstr ""

#: ../../source/clustering.rst:12
msgid "Setup Considerations"
msgstr ""

#: ../../source/clustering.rst:19
msgid "Important considerations to note in the new clustering environment:"
msgstr ""

#: ../../source/clustering.rst:21
msgid ""
"PostgreSQL is still a standalone instance node and is not clustered. "
"Tower does not manage replica configuration or database failover (if the "
"user configures standby replicas)."
msgstr ""

#: ../../source/clustering.rst:23
msgid ""
"All nodes should be reachable from all other nodes and they should be "
"able to reach the database. It is also important for the hosts to have a "
"stable address and/or hostname (depending on how the Tower host is "
"configured)."
msgstr ""

#: ../../source/clustering.rst:25
msgid ""
"RabbitMQ is the cornerstone of Tower's clustering system. A lot of the "
"configuration requirements and behavior is dictated by its needs. "
"Therefore, customization beyond Tower's setup playbook is limited. Each "
"Tower node has a deployment of RabbitMQ that will cluster with the other "
"nodes' RabbitMQ instances."
msgstr ""

#: ../../source/clustering.rst:27
msgid ""
"Existing old-style HA deployments will be migrated automatically to the "
"new HA system during the upgrade process."
msgstr ""

#: ../../source/clustering.rst:29
msgid "Manual projects must be manually synced to all nodes."
msgstr ""

#: ../../source/clustering.rst:31
msgid ""
"There is no concept of primary/secondary in the new Tower system. All "
"systems are primary."
msgstr ""

#: ../../source/clustering.rst:33
msgid ""
"Setup playbook changes to configure RabbitMQ and provide the type of "
"network the hosts are on."
msgstr ""

#: ../../source/clustering.rst:35
msgid ""
"The ``inventory`` file for Tower deployments should be saved/persisted. "
"If new nodes are to be provisioned, the passwords and configuration "
"options, as well as host names, must be made available to the installer."
msgstr ""

#: ../../source/clustering.rst:39
msgid "Install and Configure"
msgstr ""

#: ../../source/clustering.rst:41
msgid ""
"Provisioning new nodes should be as simple as updating the ``inventory`` "
"file and re-running the setup playbook. It is important that this file "
"contain all passwords and information used when installing the cluster or"
" other nodes may be reconfigured. The current standalone node "
"configuration does not change for a 3.1 deployment. The ``inventory`` "
"file does change in some important ways:"
msgstr ""

#: ../../source/clustering.rst:43
msgid ""
"Since there is no primary/secondary configuration, those inventory groups"
" go away and are replaced with a single inventory group, ``tower``. The "
"database group remains for specifying an external Postgres, however:"
msgstr ""

#: ../../source/clustering.rst:56
msgid "The ``redis_password`` field is removed from ``[all:vars]``"
msgstr ""

#: ../../source/clustering.rst:58
msgid "New fields for RabbitMQ are as follows:"
msgstr ""

#: ../../source/clustering.rst:60
msgid ""
"``rabbitmq_port=5672``: RabbitMQ is installed on each node and is not "
"optional, it's also not possible to externalize it. This setting "
"configures what port it listens on."
msgstr ""

#: ../../source/clustering.rst:62
msgid ""
"``rabbitmq_vhost=tower``: Controls the setting for which Tower configures"
" a RabbitMQ virtualhost to isolate itself."
msgstr ""

#: ../../source/clustering.rst:64
msgid ""
"``rabbitmq_username=tower`` and ``rabbitmq_password=tower``: Each node "
"and and each node's Tower instance are configured with these values. This"
" is similar to Tower's other uses of usernames/passwords."
msgstr ""

#: ../../source/clustering.rst:66
msgid ""
"``rabbitmq_cookie=<somevalue>``: This value is unused in a standalone "
"deployment but is critical for clustered deployments. This acts as the "
"secret key that allows RabbitMQ cluster members to identify each other."
msgstr ""

#: ../../source/clustering.rst:68
msgid ""
"``rabbitmq_use_long_names`` : RabbitMQ is sensitive to what each node is "
"named. Tower is flexible enough to allow FQDNs (host01.example.com), "
"short names (host01), or ip addresses (192.168.5.73). Depending on what "
"is used to identify each host in the inventory file, this value may need "
"to be changed:"
msgstr ""

#: ../../source/clustering.rst:70
msgid "For FQDNs and ip addresses, this value needs to be ``true``."
msgstr ""

#: ../../source/clustering.rst:71
msgid "For short names, set the value to ``false``."
msgstr ""

#: ../../source/clustering.rst:73
msgid ""
"``rabbitmq_enable_manager``: Set this to true to expose the RabbitMQ "
"Management Web Console on each node."
msgstr ""

#: ../../source/clustering.rst:77
msgid "RabbitMQ Default Settings"
msgstr ""

#: ../../source/clustering.rst:79
msgid "The following configuration shows the default settings for RabbitMQ:"
msgstr ""

#: ../../source/clustering.rst:95
msgid "Nodes and Ports Used by Tower"
msgstr ""

#: ../../source/clustering.rst:97
msgid "Ports and nodes used by Tower are as follows:"
msgstr ""

#: ../../source/clustering.rst:99
msgid "80, 443 (normal Tower ports)"
msgstr ""

#: ../../source/clustering.rst:101
msgid "22 (ssh)"
msgstr ""

#: ../../source/clustering.rst:103
msgid ""
"5432 (database node - if the database is installed on an external node, "
"needs to be opened to the tower nodes)"
msgstr ""

#: ../../source/clustering.rst:106
msgid "Clustering/RabbitMQ ports:"
msgstr ""

#: ../../source/clustering.rst:108
msgid ""
"4369, 25762 (ports specifically used by RabbitMQ to maintain a cluster, "
"needs to be open between each node)"
msgstr ""

#: ../../source/clustering.rst:111
msgid ""
"15672 (if the RabbitMQ Management Interface is enabled, this port needs "
"to be opened (optional))"
msgstr ""

#: ../../source/clustering.rst:116
msgid "Status and Monitoring via Browser API"
msgstr ""

#: ../../source/clustering.rst:118
msgid ""
"Tower itself reports as much status as it can via the Browsable API at "
"``/api/v1/ping`` in order to provide validation of the health of the "
"cluster, including:"
msgstr ""

#: ../../source/clustering.rst:120
msgid "The node servicing the HTTP request"
msgstr ""

#: ../../source/clustering.rst:122
msgid "The timestamps of the last heartbeat of all other nodes in the cluster"
msgstr ""

#: ../../source/clustering.rst:124
msgid "The state of the Job Queue, any jobs each node is running"
msgstr ""

#: ../../source/clustering.rst:126
msgid "The RabbitMQ cluster status"
msgstr ""

#: ../../source/clustering.rst:130
msgid "Node Services and Failure Behavior"
msgstr ""

#: ../../source/clustering.rst:132
msgid ""
"Each Tower node is made up of several different services working "
"collaboratively:"
msgstr ""

#: ../../source/clustering.rst:134
msgid ""
"HTTP Services - This includes the Tower application itself as well as "
"external web services."
msgstr ""

#: ../../source/clustering.rst:136
msgid "Callback Receiver - Receives job events from running Ansible jobs."
msgstr ""

#: ../../source/clustering.rst:138
msgid "Celery - The worker queue that processes and runs all jobs."
msgstr ""

#: ../../source/clustering.rst:140
msgid ""
"RabbitMQ - This message broker is used as a signaling mechanism for "
"Celery as well as any event data propogated to the application."
msgstr ""

#: ../../source/clustering.rst:142
msgid "Memcached - local caching service for the node it lives on."
msgstr ""

#: ../../source/clustering.rst:144
msgid ""
"Tower is configured in such a way that if any of these services or their "
"components fail, then all services are restarted. If these fail "
"sufficiently often in a short span of time, then the entire node will be "
"placed offline in an automated fashion in order to allow remediation "
"without causing unexpected behavior."
msgstr ""

#: ../../source/clustering.rst:148
msgid "Job Runtime Behavior"
msgstr ""

#: ../../source/clustering.rst:150
msgid ""
"The way jobs are run and reported to a 'normal' user of Tower does not "
"change. On the system side, some differences are worth noting:"
msgstr ""

#: ../../source/clustering.rst:152
msgid ""
"When a job is submitted from the API interface it gets pushed into the "
"Celery queue on RabbitMQ. A single RabbitMQ node is the responsible "
"master for individual queues but each Tower node will connect to and "
"receive jobs from that queue using a particular scheduling algorithm. Any"
" node in the cluster is just as likely to receive the work and execute "
"the task. If a node fails while executing jobs, then the work is marked "
"as permanently failed."
msgstr ""

#: ../../source/clustering.rst:154
msgid "|Tower Cluster example|"
msgstr ""

#: ../../source/clustering.rst:159
msgid ""
"As Tower nodes are brought online, it effectively expands the work "
"capacity of the Tower system which is measured as one entire unit (the "
"cluster's capacity). Conversely, de-provisioning a node will remove "
"capacity from the cluster. See :ref:`ag_cluster_deprovision` in the next "
"section for more details."
msgstr ""

#: ../../source/clustering.rst:162
msgid "Not all nodes are required to be provisioned with an equal capacity."
msgstr ""

#: ../../source/clustering.rst:164
msgid ""
"Project updates behave differently than they did before. Previously, they"
" were ordinary jobs that ran on a single node. It's now important that "
"they run successfully on any node that could potentially run a job. "
"Projects will now sync themselves to the correct version on the node "
"immediately prior to running the job."
msgstr ""

#: ../../source/clustering.rst:170
msgid "Deprovision Nodes"
msgstr ""

#: ../../source/clustering.rst:175
msgid ""
"Deprovisioning Tower does not automatically deprovision nodes since "
"clusters do not currently distinguish between a node that was taken "
"offline intentionally or due to failure. Instead, shutdown all services "
"on the Tower node and then run the deprovisioning tool from any other "
"node:"
msgstr ""

#: ../../source/clustering.rst:177
msgid "Shut down the node or stop the ``ansible-tower-service``."
msgstr ""

#: ../../source/clustering.rst:179
msgid ""
"Run the deprovision command ``$ tower-manage deprovision_node —name=<name"
" used in inventory file>`` from another node to remove it from the Tower "
"cluster registry AND the RabbitMQ cluster reigstry."
msgstr ""

#: ../../source/clustering.rst:181
msgid "Example: ``tower-manage deprovision_node —name=hostB``"
msgstr ""

